# May 30, 2018 - EurekaDNS

Goal: Implement tests for EurekaDNS implementation

Woah. Okay. I spent last night noodling on this implementation and I've come up
with the following thought:

We have another process in our container since we don't have pods. This is less
than desirable, but we make do with what we have. BUT we are now _investing_ in
this pattern by tying our healthcheck to the deamon's. This is causing us to
conflate infrastructure level concerns with application level concerns and may
make future work much harder. For example, if we want to take infrastructure
level action against our healthcheck, it is no longer clear what the correct
action should be. If the healthech for the NodeQuark app is unhealthy, we
should terminate the container and bring up a new healthy container at an
infrastructure level. But if the daemon is the problem, _it_ is what needs to
be restarted not the NodeQuark app. This means we sever active customer
connections for no reason. If, instead, we let the application manage the
lifecycle of the daemon, restarting it if necessary, we can no longer take
infrastructure level action against NodeQuark based on it's healthcheck metric.
In this second scenario, when NodeQuark is unhealthy, we can't assume a restart
is the correct thing to do at the infrastructure level since the application
level may already be trying to correct the problem. This conflation, while
subtle, may end up impacting customers in a real way. I'm going to finish up
the tests and open a PR, but I want to get some folks together and talk about
the decisions we are making.

Another possible solution:

Can DNS be clustered for HA?

If so, we can manage our own external eurekaDNS cluster that handles restarts
on its own. While this isn't a "pod" it is a more centralized service that we
control and whose health we can manage independently of our application. This
seems to offer a more robust way of handling infrastructure level concerns at
the infrastructure level.

An alternative solution:

Can we expand the healthcheck logic to automatically attempt a restart of the
daemon during the healthcheck, and only report a failure if we can't get a
healthy reading after a restart? This is still conflating infrastructure level
concerns at the application level but avoids centralizing a DNS service.

These are things I need to noodle on more, and will discuss with some other
people, but for now let's just finish up the simple healthcheck and go from
there.

... break ...

I bounced this off of my manager. She suggested I create a document to share
with the team. Once we align as a team, we can use it to drive conversations
with other teams (i.e. Eureka) if that is necessary. Going to copy the above
paragraphs over, yay for keeping a journal!

... writing ...

Okay, that is written up. Before broadcasting to the team I'm going to finish
up these tests so we have code to reference while talking about things.

Todo:

* Test eurekaDNS logic
  * proxyquire http
  * ensure errors are returned on correct conditions:
    * 500
    * Error
  * ensure success is returned on 200
* Test healthcheck logic
  * proxyquire eurekaDNS healthcheck and ensure:
    * 200 on success
    * 500 on error

... coding ...

Alright, first step is to take the existing healthcheck tests and figure out which ones should be kept as-is and which ones need to be modified. From the outside, the healthcheck should behave the same as before with the addition of error cases.

I commented out all of the tests that _invoke_ the healthcheck for now since those need mocks. But there is a function that just ensures the returned handler is a function, lets keep that and try to run the tests using `describe.only` to see if the code runs without errors.

`make test`

... waiting for yarn etc. ...

Well yarn stalled for like an hour and froze up my laptop. I need to get an
SSD. I have been thinking of switching to the Lenovo X200 for a while now since
its docking station works better with Linux, it has a smaller form factor, and
a few other nice things. I may buy one with an SSD pre-installed and do a fresh
Trisquel install on it in the near future. I'm not sure I can do Node.js dev on
this due to the node_modules folders being so large, my poor disk can't keep
up. If this sounds strange, it is, but I'm using an oooold magnetic disk drive
with really slow read/writes and it just can't keep up with what newer dev
toolchains expect from a filesystem. Oh well, lets try this again.


... waiting for yarn ...

Worked this time! Now:
`make test`

And now I get:

```
healthcheck is not defined
```

.. fixing ..

Oh,

```
let healthcheck = '../path/to/file`
...
let healthcheck = proxyquire(healthcheck...
```

isn't going to work heh, fixing that

and `make test`

Oh, that runs integration tests too

We want `make test-unit`

:boom: passes. Now lets start mocking stuff.

... coding ...

Looking up API for `process.nextTick`... Oh handy for success I just need
`process.nextTick(cb)`. I just want to make sure I break the event loop like
the actual handler would.

And the test stalls... Something isn't calling a callback!

... adding console.error statements ...

Oh, I'm calling the constructor:

```
let healthcheck = proxyquire(...);
...
healtcheck(REQ, RES, NEXT);
```

It should be:

```
let healthcheck = proxyquire(...)();
...
healtcheck(REQ, RES, NEXT);
```

Oh, and I need to pass an options object. Oh and I need to require `http`.

Huh looks like my proxyquire isn't working... Oh, the path should be relative
to the file using the mock, not the test.

And success!

Now we need to test the error paths. This mock should return an error.

I need to see what the error message should look like, adding a console.error
statement...

`Error: EFAKEERROR`

Cool... Oh I'm returning 200 all the time, need to return the statusCode I derived from the callback response...

And we are good!

Onto testing the other module.

From above we need to:

* proxyquire http
* ensure errors are returned on correct conditions:
  * 500
  * Error
* ensure success is returned on 200

... coding ...

And we are done!

All that is left is linting...

... And we are good! Let's run the full test suite now....

Ah! We forgot to add the config option in the server's code. Let's do that.

And we need to spin up a fake EurekaDNS healthcheck endpoint for some of the
tests, lets do that.

...

And we are good! Sending a PR!

...

Calling it a night.
